{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "_change_revision": 3,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "classifying-tasks-using-eeg-data-w-tensorflow-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldomanpi/nnpainsensor/blob/master/classifying_tasks_using_eeg_data_w_tensorflow_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "61384bb0-de67-7c30-51be-a79339cfac31",
        "id": "auDm-Qf_qH1n",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we'll try to use Tensorflow to classify the state of the subject according to raw EEG data. I use Nick Merril's Kernel to help guide my data cleaning. We try two tasks, here\n",
        "\n",
        " 1. Classify task label by EEG power for 1 subject, only.\n",
        " 2. Classify task label by EEG power for all subjects. This is harder, as it tries to use EEG Power data (which is unique to each person) to create a general classification rule for everyone.\n",
        "\n",
        "https://www.kaggle.com/elsehow/d/berkeley-biosense/synchronized-brainwave-dataset/classifying-relaxation-versus-doing-math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "756b8617-ac51-efb6-80d9-0f381cb26536",
        "id": "HC_t1E1lqH1p",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Classify tasks for just one subject. ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "dfb6a238-7b67-0d51-d3c9-faedb3c1e5b8",
        "id": "W41WyEKZqH1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "10db6a5a-5510-46e3-8ede-a0c393fcceb3"
      },
      "source": [
        "url ='https://github.com/aldomanpi/nnpainsensor/blob/4ce50563351e0f2b9817afa058f1478ea0a594f5/eeg-data.csv?raw=true' \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import json\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "# convert to arrays from strings\n",
        "df['eeg_power'] = df.eeg_power.map(json.loads)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ald/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9e75e42e-6c59-72cc-f282-8a3c449e1313",
        "id": "E6VdcTXSqH1z",
        "colab_type": "code",
        "outputId": "9b2868d6-ec27-4d2d-83ba-67d4792e908b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = df.drop('Unnamed: 0', 1)\n",
        "df = df.drop('indra_time', 1)\n",
        "df = df.drop('browser_latency', 1)\n",
        "df = df.drop('reading_time', 1)\n",
        "df = df.drop('attention_esense', 1)\n",
        "df = df.drop('meditation_esense', 1)\n",
        "df = df.drop('raw_values', 1)\n",
        "df = df.drop('signal_quality', 1)\n",
        "df = df.drop('createdAt', 1)\n",
        "df = df.drop('updatedAt', 1)\n",
        "\n",
        "print(df.columns.values)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id' 'eeg_power' 'label']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "df72a89a-2d3a-94af-f8fc-4ed06a9a7894",
        "id": "SomxkrSgqH14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate eeg power to multiple columns\n",
        "to_series = pd.Series(df['eeg_power']) # df to series\n",
        "eeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n",
        "df = pd.concat([df,eeg_features], axis=1) # concatenate the create columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d6fbe747-6123-0292-f61b-489ea8505b6f",
        "id": "DF94yWZvqH18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just look at first subject\n",
        "df=df.loc[df['id'] == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1040721b-ff18-355b-5a20-d9f51885809a",
        "id": "2txe6eovqH2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df = df.drop('eeg_power', 1) # drop comma separated cell\n",
        "df = df.drop('id', 1) # drop comma separated cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5d9287f6-6686-ece9-799c-02719ab4bc61",
        "id": "dCb44uY3qH2G",
        "colab_type": "code",
        "outputId": "c9bfae76-5304-444e-a27f-f70de9ab4d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# prepare for training\n",
        "label=df.pop(\"label\") # pop off labels to new group\n",
        "print(df.shape)\n",
        "print(df.head())\n",
        "# convert to np array. df has our featuers\n",
        "df=df.values\n",
        "\n",
        "\n",
        "\n",
        "# convert labels to onehots \n",
        "train_labels = pd.get_dummies(label)\n",
        "# make np array\n",
        "train_labels = train_labels.values\n",
        "print(train_labels.shape)\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n",
        "# so now we have predictors and y values, separated into test and train\n",
        "\n",
        "x_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(940, 8)\n",
            "              0         1        2        3        4        5        6  \\\n",
            "4857        0.0       1.0      0.0      0.0      0.0      0.0      0.0   \n",
            "4870       75.0       4.0      3.0      0.0      0.0      0.0      1.0   \n",
            "4885  2228068.0  475505.0  28247.0  58551.0  27508.0  37206.0  34819.0   \n",
            "4899  1171204.0   28198.0  12255.0  13859.0   9756.0   8150.0   4567.0   \n",
            "4913   180187.0   54984.0  15517.0  17104.0  15716.0  15863.0  33820.0   \n",
            "\n",
            "            7  \n",
            "4857      0.0  \n",
            "4870      1.0  \n",
            "4885  16430.0  \n",
            "4899   5666.0  \n",
            "4913  11957.0  \n",
            "(940, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "6db35222-1c2e-edb4-28ad-72c385accd7a",
        "id": "KZN7LUweqH2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# place holder for inputs. feed in later\n",
        "x = tf.placeholder( tf.float32, [None, 8])\n",
        "# # # take 20 features  to 10 nodes in hidden layer\n",
        "w1 = tf.Variable(tf.random_normal([8, 1000],stddev=.5,name='w1'))\n",
        "# # # add biases for each node\n",
        "b1 = tf.Variable(tf.zeros([1000]))\n",
        "# # calculate activations \n",
        "hidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n",
        "w2 = tf.Variable(tf.random_normal([1000, 65],stddev=.5,name='w2'))\n",
        "b2 = tf.Variable(tf.zeros([65]))\n",
        "\n",
        "# # placeholder for correct values \n",
        "y_ = tf.placeholder(\"float\", [None,65])\n",
        "# # #implement model. these are predicted ys\n",
        "y = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3db5c116-f2a7-19a5-4d20-04cb0dfc94d7",
        "id": "mNUF7kolqH2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(y, y_, name='xentropy')))\n",
        "opt = tf.train.AdamOptimizer(learning_rate=.002)\n",
        "train_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "bd5bd436-1e7f-0b09-a2a2-0464ad5a46c1",
        "id": "CBrk84I7qH2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mini_batch(x,y):\n",
        "\trows=np.random.choice(x.shape[0], 50)\n",
        "\treturn x[rows], y[rows]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c6a624db-d380-4865-625c-42b296b93f1d",
        "id": "zZWCuTrLqH2c",
        "colab_type": "code",
        "outputId": "e670e655-d1b6-4381-99fa-e78f872870c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# start session\n",
        "sess = tf.Session()\n",
        "# init all vars\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ald/.local/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8afc8598-9068-7c64-d1c1-e702a452ab49",
        "id": "jyRj527AqH2i",
        "colab_type": "code",
        "outputId": "80aee123-5b28-4fd8-fde3-46d225e58ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "ntrials = 10000\n",
        "for i in range(ntrials):\n",
        "    # get mini batch\n",
        "    a,b=get_mini_batch(x_train,y_train)\n",
        "    # run train step, feeding arrays of 100 rows each time\n",
        "    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n",
        "    if i%500 ==0:\n",
        "    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "print(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0 and cost is 209.42552185058594\n",
            "epoch is 500 and cost is 185.27215576171875\n",
            "epoch is 1000 and cost is 185.68637084960938\n",
            "epoch is 1500 and cost is 177.75477600097656\n",
            "epoch is 2000 and cost is 179.3441925048828\n",
            "epoch is 2500 and cost is 172.23660278320312\n",
            "epoch is 3000 and cost is 176.13282775878906\n",
            "epoch is 3500 and cost is 180.62429809570312\n",
            "epoch is 4000 and cost is 174.10279846191406\n",
            "epoch is 4500 and cost is 168.27796936035156\n",
            "epoch is 5000 and cost is 176.06459045410156\n",
            "epoch is 5500 and cost is 184.04139709472656\n",
            "epoch is 6000 and cost is 172.1102294921875\n",
            "epoch is 6500 and cost is 174.0480499267578\n",
            "epoch is 7000 and cost is 171.08175659179688\n",
            "epoch is 7500 and cost is 181.03579711914062\n",
            "epoch is 8000 and cost is 177.0333709716797\n",
            "epoch is 8500 and cost is 179.030029296875\n",
            "epoch is 9000 and cost is 178.02867126464844\n",
            "epoch is 9500 and cost is 176.02774047851562\n",
            "test accuracy is 0.6755319237709045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "708787df-0eae-47ea-2518-bdca36e2ebbd",
        "id": "grMez74OqH2n",
        "colab_type": "code",
        "outputId": "3eaa7f74-6941-404f-a662-08bc26dac766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sess.close"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseSession.close of <tensorflow.python.client.session.Session object at 0x7f8350494400>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf465aff-359a-1afb-a1c4-678565247c5a",
        "id": "ou7Z2_5fqH2x",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Classify tasks for all subjects. ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "889acb97-6940-b332-4106-fd736488f3e4",
        "id": "Gv83cyd4qH2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://github.com/aldomanpi/nnpainsensor/blob/4ce50563351e0f2b9817afa058f1478ea0a594f5/eeg-data.csv?raw=true'\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import json\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "# convert to arrays from strings\n",
        "df['eeg_power'] = df.eeg_power.map(json.loads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "54307894-b64d-8531-a59e-b18a37f41b3b",
        "id": "uDUMnV12qH22",
        "colab_type": "text"
      },
      "source": [
        "Now we clean the dataset, and drop most of the columns that we won't use in this analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "47388d69-0d6d-cfef-ea0d-d10416f4a12f",
        "id": "j5BYui8aqH23",
        "colab_type": "code",
        "outputId": "f905138a-fb86-4227-cd64-b1e2dccab853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = df.drop('Unnamed: 0', 1)\n",
        "df = df.drop('id', 1)\n",
        "df = df.drop('indra_time', 1)\n",
        "df = df.drop('browser_latency', 1)\n",
        "df = df.drop('reading_time', 1)\n",
        "df = df.drop('attention_esense', 1)\n",
        "df = df.drop('meditation_esense', 1)\n",
        "df = df.drop('raw_values', 1)\n",
        "df = df.drop('signal_quality', 1)\n",
        "df = df.drop('createdAt', 1)\n",
        "df = df.drop('updatedAt', 1)\n",
        "\n",
        "print(df.columns.values)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eeg_power' 'label']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "749aa8c7-e622-5fa7-12db-69801ae752e5",
        "id": "f2VPL8FgqH3B",
        "colab_type": "text"
      },
      "source": [
        "eeg_power contains data on  8 commonly-recognized types of EEG frequency bands...these are comma separated, but we need to create new columns for each band (delta (0.5 - 2.75Hz), theta (3.5 - 6.75Hz), low-alpha (7.5 - 9.25Hz), high-alpha (10 - 11.75Hz), low-beta (13 - 16.75Hz), high-beta (18 - 29.75Hz), low-gamma (31 - 39.75Hz), and mid-gamma (41 - 49.75Hz))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "16f37c88-5e2a-e79d-f401-d53645438c74",
        "id": "uv_kNA2KqH3C",
        "colab_type": "code",
        "outputId": "3649c3e3-1681-410e-ad3b-26c693bd0fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "to_series = pd.Series(df['eeg_power']) # df to series\n",
        "eeg_cols=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n",
        "print(eeg_cols.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           0         1         2         3         4         5         6  \\\n",
            "0   944412.0  111373.0   52404.0   28390.0    3237.0   32728.0    4845.0   \n",
            "1  1793049.0   89551.0    3896.0   21727.0    9301.0   16096.0    3496.0   \n",
            "2   400192.0  640624.0  153087.0   69733.0   98854.0  199537.0   66993.0   \n",
            "3   681192.0  138630.0   67891.0   26459.0  592240.0  171435.0  164399.0   \n",
            "4   268406.0  197772.0  190654.0  266433.0   91683.0  200452.0  107585.0   \n",
            "\n",
            "         7  \n",
            "0   2036.0  \n",
            "1    643.0  \n",
            "2  51772.0  \n",
            "3  41765.0  \n",
            "4  57841.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7a4f9e31-fedf-8e81-43c5-4f63d5e1926c",
        "id": "vGvBi63YqH3G",
        "colab_type": "text"
      },
      "source": [
        "This works. We have the 8 variables split into 8 distinct columns. Nice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "eca31719-3116-d233-9bda-ec8fe44596d7",
        "id": "8avweoD3qH3H",
        "colab_type": "code",
        "outputId": "993e1fde-a78a-4d2c-d88a-5fb8c2be08ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df = pd.concat([df,eeg_cols], axis=1, join='outer') # concatenate the create columns\n",
        "df = df.drop('eeg_power', 1) # drop comma separated cell\n",
        "print(df.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       label          0         1         2         3         4         5  \\\n",
            "0  unlabeled   944412.0  111373.0   52404.0   28390.0    3237.0   32728.0   \n",
            "1  unlabeled  1793049.0   89551.0    3896.0   21727.0    9301.0   16096.0   \n",
            "2  unlabeled   400192.0  640624.0  153087.0   69733.0   98854.0  199537.0   \n",
            "3  unlabeled   681192.0  138630.0   67891.0   26459.0  592240.0  171435.0   \n",
            "4  unlabeled   268406.0  197772.0  190654.0  266433.0   91683.0  200452.0   \n",
            "\n",
            "          6        7  \n",
            "0    4845.0   2036.0  \n",
            "1    3496.0    643.0  \n",
            "2   66993.0  51772.0  \n",
            "3  164399.0  41765.0  \n",
            "4  107585.0  57841.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a60b4a18-b175-14fd-1c93-7a0c6cb7f8b4",
        "id": "Q9YkT-o0qH3N",
        "colab_type": "text"
      },
      "source": [
        "We have a dataframe that we can now split into test and train sets and do train a NN on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f52f5ba0-106d-d77f-fb95-a939693dca3b",
        "id": "dEE2z5nVqH3O",
        "colab_type": "code",
        "outputId": "00217bd7-3895-4554-9c7d-1e04c72503d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare for training\n",
        "label=df.pop(\"label\") # pop off labels to new group\n",
        "print(\"the df of features now as shape{0} and the label set has shape {1}\".format(df.shape,label.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the df of features now as shape(30013, 8) and the label set has shape (30013,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a65224b2-c4c6-29e8-d638-10234b004aa5",
        "id": "rb7-nba4qH3T",
        "colab_type": "text"
      },
      "source": [
        "We convert these two sets to np arrays to TF training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "664f3387-f761-1ccf-7c0e-46808d2c47ae",
        "id": "QnAv1KCWqH3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to np array. df has our featuers\n",
        "df=df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "808f3cc8-d774-87a3-135f-64787cc2264d",
        "id": "EERrgCLhqH3X",
        "colab_type": "code",
        "outputId": "0a0750bf-8290-4f10-eccb-098f16d7b5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# convert labels to onehots \n",
        "train_labels = pd.get_dummies(label)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30013, 68)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6cdb4e70-d27f-61fc-dcb5-b0fa88c40ddc",
        "id": "MsyuB5IgqH3c",
        "colab_type": "text"
      },
      "source": [
        "There are 69 different tasks classified by the researchers. There is redundancy here (blink 1 is distinct from blink 2...this will complicate our ability to correctly classify the various task labels, but let's just stick with it for now.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "aa4e52d5-b9ab-2bab-f42f-90561b7c9920",
        "id": "9hKl8ZyhqH3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert train_labels to np array, too\n",
        "train_labels = train_labels.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ec48930a-4412-cf90-1a8b-4eb1e602f821",
        "id": "876ZpjQFqH3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use sklearn to split for training\n",
        "x_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n",
        "x_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7e291008-16c0-7cca-4de4-3fc9ebfd3715",
        "id": "yP1yUaUPqH3k",
        "colab_type": "text"
      },
      "source": [
        "Now, let's do some tensorflow and build a simple model with 1 hidden layer with 1000 nodes in this layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79d080d7-e265-6540-c1e1-e20ce3f28a39",
        "id": "4dYzyQY9qH3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, 8])\n",
        "w1 = tf.Variable(tf.random_normal([8, 1000],stddev=.5,name='w1'))\n",
        "b1 = tf.Variable(tf.zeros([1000]))\n",
        "# # calculate hidden output\n",
        "hidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n",
        "# bring from 1000 nodes to one of 69 possible labels\n",
        "w2 = tf.Variable(tf.random_normal([1000, 68],stddev=.5,name='w2'))\n",
        "b2 = tf.Variable(tf.zeros([68]))\n",
        "# # placeholder for correct values \n",
        "y_ = tf.placeholder(\"float\", [None,68])\n",
        "# # #implement model. these are predicted ys\n",
        "y = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c4ab3def-31bf-c761-8742-164b4c089c35",
        "id": "j8txxMnaqH3p",
        "colab_type": "text"
      },
      "source": [
        "Prepare the training. Use ADAM optimizer to adjust learning rate over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2bcb024b-28b9-70d2-40a0-de4a976ee59c",
        "id": "cSvd_zkZqH3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(y, y_, name='xentropy')))\n",
        "opt = tf.train.AdamOptimizer(learning_rate=.005)\n",
        "train_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "967c7624-76c8-cb5e-67be-de4fd0c4eb1e",
        "id": "kZe0aUugqH3w",
        "colab_type": "text"
      },
      "source": [
        "Create a function to get mini_batch, so that we aren't feeding data in every training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ebda287b-b6c1-fcb8-321e-897a9cdd939b",
        "id": "N-EuVr3BqH3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mini_batch(x,y):\n",
        "\trows=np.random.choice(x.shape[0], 100)\n",
        "\treturn x[rows], y[rows]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8029bf1c-069b-0803-6367-50dd7877fd6b",
        "id": "nMlhMoAIqH32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "# init all vars in graph\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e168321-f072-8d72-b5d5-e578fb0b79e0",
        "id": "qNMduvVZqH35",
        "colab_type": "text"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d27eb3c1-e826-0262-5033-78d5ee98ee26",
        "id": "7nFmp4YWqH36",
        "colab_type": "code",
        "outputId": "5cbbf358-4e0e-4d80-b6ee-83aae3b6fa88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ntrials = 1000000\n",
        "for i in range(ntrials):\n",
        "    # get mini batch\n",
        "    a,b=get_mini_batch(x_train,y_train)\n",
        "    # run train step, feeding arrays of 100 rows each time\n",
        "    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n",
        "    if i%500 ==0:\n",
        "    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "print(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0 and cost is 423.0769958496094\n",
            "epoch is 500 and cost is 363.3647766113281\n",
            "epoch is 1000 and cost is 355.54705810546875\n",
            "epoch is 1500 and cost is 357.51495361328125\n",
            "epoch is 2000 and cost is 365.4857177734375\n",
            "epoch is 2500 and cost is 356.4495544433594\n",
            "epoch is 3000 and cost is 353.45672607421875\n",
            "epoch is 3500 and cost is 359.44757080078125\n",
            "epoch is 4000 and cost is 362.447998046875\n",
            "epoch is 4500 and cost is 356.4469299316406\n",
            "epoch is 5000 and cost is 363.44635009765625\n",
            "epoch is 5500 and cost is 352.4462585449219\n",
            "epoch is 6000 and cost is 359.4462890625\n",
            "epoch is 6500 and cost is 353.44622802734375\n",
            "epoch is 7000 and cost is 355.44622802734375\n",
            "epoch is 7500 and cost is 357.44622802734375\n",
            "epoch is 8000 and cost is 350.44622802734375\n",
            "epoch is 8500 and cost is 358.44622802734375\n",
            "epoch is 9000 and cost is 359.44622802734375\n",
            "epoch is 9500 and cost is 353.44622802734375\n",
            "epoch is 10000 and cost is 351.44622802734375\n",
            "epoch is 10500 and cost is 355.44622802734375\n",
            "epoch is 11000 and cost is 359.44622802734375\n",
            "epoch is 11500 and cost is 351.44622802734375\n",
            "epoch is 12000 and cost is 360.44622802734375\n",
            "epoch is 12500 and cost is 357.44622802734375\n",
            "epoch is 13000 and cost is 356.44622802734375\n",
            "epoch is 13500 and cost is 358.44622802734375\n",
            "epoch is 14000 and cost is 352.44622802734375\n",
            "epoch is 14500 and cost is 350.44622802734375\n",
            "epoch is 15000 and cost is 356.44622802734375\n",
            "epoch is 15500 and cost is 350.44622802734375\n",
            "epoch is 16000 and cost is 355.44622802734375\n",
            "epoch is 16500 and cost is 358.44622802734375\n",
            "epoch is 17000 and cost is 357.44622802734375\n",
            "epoch is 17500 and cost is 353.44622802734375\n",
            "epoch is 18000 and cost is 352.44622802734375\n",
            "epoch is 18500 and cost is 348.44622802734375\n",
            "epoch is 19000 and cost is 363.44622802734375\n",
            "epoch is 19500 and cost is 358.44622802734375\n",
            "epoch is 20000 and cost is 360.44622802734375\n",
            "epoch is 20500 and cost is 346.44622802734375\n",
            "epoch is 21000 and cost is 347.44622802734375\n",
            "epoch is 21500 and cost is 358.44622802734375\n",
            "epoch is 22000 and cost is 353.44622802734375\n",
            "epoch is 22500 and cost is 353.44622802734375\n",
            "epoch is 23000 and cost is 356.44622802734375\n",
            "epoch is 23500 and cost is 356.44622802734375\n",
            "epoch is 24000 and cost is 362.44622802734375\n",
            "epoch is 24500 and cost is 356.44622802734375\n",
            "epoch is 25000 and cost is 348.44622802734375\n",
            "epoch is 25500 and cost is 357.44622802734375\n",
            "epoch is 26000 and cost is 361.44622802734375\n",
            "epoch is 26500 and cost is 353.44622802734375\n",
            "epoch is 27000 and cost is 350.44622802734375\n",
            "epoch is 27500 and cost is 361.44622802734375\n",
            "epoch is 28000 and cost is 352.44622802734375\n",
            "epoch is 28500 and cost is 360.44622802734375\n",
            "epoch is 29000 and cost is 347.44622802734375\n",
            "epoch is 29500 and cost is 358.44622802734375\n",
            "epoch is 30000 and cost is 354.44622802734375\n",
            "epoch is 30500 and cost is 361.44622802734375\n",
            "epoch is 31000 and cost is 354.44622802734375\n",
            "epoch is 31500 and cost is 351.44622802734375\n",
            "epoch is 32000 and cost is 355.44622802734375\n",
            "epoch is 32500 and cost is 350.44622802734375\n",
            "epoch is 33000 and cost is 347.44622802734375\n",
            "epoch is 33500 and cost is 355.44622802734375\n",
            "epoch is 34000 and cost is 355.44622802734375\n",
            "epoch is 34500 and cost is 358.44622802734375\n",
            "epoch is 35000 and cost is 354.44622802734375\n",
            "epoch is 35500 and cost is 359.44622802734375\n",
            "epoch is 36000 and cost is 361.44622802734375\n",
            "epoch is 36500 and cost is 363.44622802734375\n",
            "epoch is 37000 and cost is 369.44622802734375\n",
            "epoch is 37500 and cost is 355.44622802734375\n",
            "epoch is 38000 and cost is 357.44622802734375\n",
            "epoch is 38500 and cost is 348.44622802734375\n",
            "epoch is 39000 and cost is 355.44622802734375\n",
            "epoch is 39500 and cost is 361.44622802734375\n",
            "epoch is 40000 and cost is 355.44622802734375\n",
            "epoch is 40500 and cost is 356.44622802734375\n",
            "epoch is 41000 and cost is 353.44622802734375\n",
            "epoch is 41500 and cost is 356.44622802734375\n",
            "epoch is 42000 and cost is 354.44622802734375\n",
            "epoch is 42500 and cost is 350.44622802734375\n",
            "epoch is 43000 and cost is 346.44622802734375\n",
            "epoch is 43500 and cost is 353.44622802734375\n",
            "epoch is 44000 and cost is 358.44622802734375\n",
            "epoch is 44500 and cost is 360.44622802734375\n",
            "epoch is 45000 and cost is 363.44622802734375\n",
            "epoch is 45500 and cost is 369.44622802734375\n",
            "epoch is 46000 and cost is 359.44622802734375\n",
            "epoch is 46500 and cost is 367.44622802734375\n",
            "epoch is 47000 and cost is 347.44622802734375\n",
            "epoch is 47500 and cost is 354.44622802734375\n",
            "epoch is 48000 and cost is 348.44622802734375\n",
            "epoch is 48500 and cost is 348.44622802734375\n",
            "epoch is 49000 and cost is 358.44622802734375\n",
            "epoch is 49500 and cost is 359.44622802734375\n",
            "epoch is 50000 and cost is 358.44622802734375\n",
            "epoch is 50500 and cost is 358.44622802734375\n",
            "epoch is 51000 and cost is 359.44622802734375\n",
            "epoch is 51500 and cost is 358.44622802734375\n",
            "epoch is 52000 and cost is 360.44622802734375\n",
            "epoch is 52500 and cost is 361.44622802734375\n",
            "epoch is 53000 and cost is 360.44622802734375\n",
            "epoch is 53500 and cost is 356.44622802734375\n",
            "epoch is 54000 and cost is 352.44622802734375\n",
            "epoch is 54500 and cost is 357.44622802734375\n",
            "epoch is 55000 and cost is 353.44622802734375\n",
            "epoch is 55500 and cost is 357.44622802734375\n",
            "epoch is 56000 and cost is 351.44622802734375\n",
            "epoch is 56500 and cost is 365.44622802734375\n",
            "epoch is 57000 and cost is 361.44622802734375\n",
            "epoch is 57500 and cost is 352.44622802734375\n",
            "epoch is 58000 and cost is 366.44622802734375\n",
            "epoch is 58500 and cost is 361.44622802734375\n",
            "epoch is 59000 and cost is 359.44622802734375\n",
            "epoch is 59500 and cost is 353.44622802734375\n",
            "epoch is 60000 and cost is 351.44622802734375\n",
            "epoch is 60500 and cost is 358.44622802734375\n",
            "epoch is 61000 and cost is 356.44622802734375\n",
            "epoch is 61500 and cost is 357.44622802734375\n",
            "epoch is 62000 and cost is 353.44622802734375\n",
            "epoch is 62500 and cost is 354.44622802734375\n",
            "epoch is 63000 and cost is 353.44622802734375\n",
            "epoch is 63500 and cost is 358.44622802734375\n",
            "epoch is 64000 and cost is 359.44622802734375\n",
            "epoch is 64500 and cost is 364.44622802734375\n",
            "epoch is 65000 and cost is 353.44622802734375\n",
            "epoch is 65500 and cost is 359.44622802734375\n",
            "epoch is 66000 and cost is 360.44622802734375\n",
            "epoch is 66500 and cost is 356.44622802734375\n",
            "epoch is 67000 and cost is 352.44622802734375\n",
            "epoch is 67500 and cost is 362.44622802734375\n",
            "epoch is 68000 and cost is 364.44622802734375\n",
            "epoch is 68500 and cost is 358.44622802734375\n",
            "epoch is 69000 and cost is 357.44622802734375\n",
            "epoch is 69500 and cost is 367.44622802734375\n",
            "epoch is 70000 and cost is 364.44622802734375\n",
            "epoch is 70500 and cost is 356.44622802734375\n",
            "epoch is 71000 and cost is 352.44622802734375\n",
            "epoch is 71500 and cost is 375.44622802734375\n",
            "epoch is 72000 and cost is 352.44622802734375\n",
            "epoch is 72500 and cost is 358.44622802734375\n",
            "epoch is 73000 and cost is 366.44622802734375\n",
            "epoch is 73500 and cost is 356.44622802734375\n",
            "epoch is 74000 and cost is 350.44622802734375\n",
            "epoch is 74500 and cost is 363.44622802734375\n",
            "epoch is 75000 and cost is 354.44622802734375\n",
            "epoch is 75500 and cost is 357.44622802734375\n",
            "epoch is 76000 and cost is 361.44622802734375\n",
            "epoch is 76500 and cost is 357.44622802734375\n",
            "epoch is 77000 and cost is 356.44622802734375\n",
            "epoch is 77500 and cost is 354.44622802734375\n",
            "epoch is 78000 and cost is 354.44622802734375\n",
            "epoch is 78500 and cost is 348.44622802734375\n",
            "epoch is 79000 and cost is 360.44622802734375\n",
            "epoch is 79500 and cost is 361.44622802734375\n",
            "epoch is 80000 and cost is 358.44622802734375\n",
            "epoch is 80500 and cost is 352.44622802734375\n",
            "epoch is 81000 and cost is 360.44622802734375\n",
            "epoch is 81500 and cost is 359.44622802734375\n",
            "epoch is 82000 and cost is 357.44622802734375\n",
            "epoch is 82500 and cost is 362.44622802734375\n",
            "epoch is 83000 and cost is 350.44622802734375\n",
            "epoch is 83500 and cost is 360.44622802734375\n",
            "epoch is 84000 and cost is 355.44622802734375\n",
            "epoch is 84500 and cost is 362.44622802734375\n",
            "epoch is 85000 and cost is 351.44622802734375\n",
            "epoch is 85500 and cost is 360.44622802734375\n",
            "epoch is 86000 and cost is 356.44622802734375\n",
            "epoch is 86500 and cost is 357.44622802734375\n",
            "epoch is 87000 and cost is 360.44622802734375\n",
            "epoch is 87500 and cost is 356.44622802734375\n",
            "epoch is 88000 and cost is 358.44622802734375\n",
            "epoch is 88500 and cost is 358.44622802734375\n",
            "epoch is 89000 and cost is 359.44622802734375\n",
            "epoch is 89500 and cost is 350.44622802734375\n",
            "epoch is 90000 and cost is 362.44622802734375\n",
            "epoch is 90500 and cost is 356.44622802734375\n",
            "epoch is 91000 and cost is 356.44622802734375\n",
            "epoch is 91500 and cost is 357.44622802734375\n",
            "epoch is 92000 and cost is 353.44622802734375\n",
            "epoch is 92500 and cost is 358.44622802734375\n",
            "epoch is 93000 and cost is 362.44622802734375\n",
            "epoch is 93500 and cost is 356.44622802734375\n",
            "epoch is 94000 and cost is 351.44622802734375\n",
            "epoch is 94500 and cost is 350.44622802734375\n",
            "epoch is 95000 and cost is 360.44622802734375\n",
            "epoch is 95500 and cost is 359.44622802734375\n",
            "epoch is 96000 and cost is 356.44622802734375\n",
            "epoch is 96500 and cost is 357.44622802734375\n",
            "epoch is 97000 and cost is 363.44622802734375\n",
            "epoch is 97500 and cost is 354.44622802734375\n",
            "epoch is 98000 and cost is 358.44622802734375\n",
            "epoch is 98500 and cost is 360.44622802734375\n",
            "epoch is 99000 and cost is 358.44622802734375\n",
            "epoch is 99500 and cost is 352.44622802734375\n",
            "epoch is 100000 and cost is 353.44622802734375\n",
            "epoch is 100500 and cost is 359.44622802734375\n",
            "epoch is 101000 and cost is 359.44622802734375\n",
            "epoch is 101500 and cost is 359.44622802734375\n",
            "epoch is 102000 and cost is 367.44622802734375\n",
            "epoch is 102500 and cost is 360.44622802734375\n",
            "epoch is 103000 and cost is 361.44622802734375\n",
            "epoch is 103500 and cost is 359.44622802734375\n",
            "epoch is 104000 and cost is 351.44622802734375\n",
            "epoch is 104500 and cost is 352.44622802734375\n",
            "epoch is 105000 and cost is 352.44622802734375\n",
            "epoch is 105500 and cost is 366.44622802734375\n",
            "epoch is 106000 and cost is 356.44622802734375\n",
            "epoch is 106500 and cost is 359.44622802734375\n",
            "epoch is 107000 and cost is 362.44622802734375\n",
            "epoch is 107500 and cost is 358.44622802734375\n",
            "epoch is 108000 and cost is 359.44622802734375\n",
            "epoch is 108500 and cost is 361.44622802734375\n",
            "epoch is 109000 and cost is 358.44622802734375\n",
            "epoch is 109500 and cost is 356.44622802734375\n",
            "epoch is 110000 and cost is 356.44622802734375\n",
            "epoch is 110500 and cost is 359.44622802734375\n",
            "epoch is 111000 and cost is 360.44622802734375\n",
            "epoch is 111500 and cost is 362.44622802734375\n",
            "epoch is 112000 and cost is 363.44622802734375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "72e26dd9-faca-60e9-5cee-c58a0f7dad7b",
        "id": "0q0D-aKuqH39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b287a73c-76e2-e3e3-ab21-2d80d56cc529",
        "id": "hJcDqfVyqH4B",
        "colab_type": "text"
      },
      "source": [
        "Given that we are trying to predict one of 69 possible labels, and that there are redundant labels, this accuracy is decent. Random guessing would get this right 1/69 = 1.5%.\n",
        "\n",
        "Upon further consideration, we see that most of the data is unlabeled. For instance, for subject 1, there are 943 rows...of these 943, the label \"everyone paired\"  appears 361 times and \"unlabeled\" appears 189 times. So if we can just get these two right, we get 58% accuracy. So this is actually not nearly as good as we would think -- the non-uniformity of the distribution makes this classification problem easier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "97910ccd-c733-c636-2434-acf834278a66",
        "id": "WUnQPPGMqH4F",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Consolidate labels and run for individual subject. Compare only MATH and RELAX, similar to Nick's kernel.##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c2047866-7e4f-e671-d6fb-b725c10987cf",
        "id": "NKCSVQe-qH4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://github.com/aldomanpi/nnpainsensor/blob/4ce50563351e0f2b9817afa058f1478ea0a594f5/eeg-data.csv?raw=true'\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import json\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "def prepare_individual_data(df,individual):\n",
        "\t# drop unused features. just leave eeg_power and the label\n",
        "\tdf = df.drop('Unnamed: 0', 1)\n",
        "\t# df = df.drop('id', 1)\n",
        "\tdf = df.drop('indra_time', 1)\n",
        "\tdf = df.drop('browser_latency', 1)\n",
        "\tdf = df.drop('reading_time', 1)\n",
        "\tdf = df.drop('attention_esense', 1)\n",
        "\tdf = df.drop('meditation_esense', 1)\n",
        "\tdf = df.drop('raw_values', 1)\n",
        "\tdf = df.drop('signal_quality', 1)\n",
        "\tdf = df.drop('createdAt', 1)\n",
        "\tdf = df.drop('updatedAt', 1)\n",
        "\t# separate eeg power to multiple columns\n",
        "\tto_series = pd.Series(df['eeg_power']) # df to series\n",
        "\teeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n",
        "\tdf = pd.concat([df,eeg_features], axis=1) # concatenate the create columns\n",
        "\t# df = pd.concat([df,eeg_features], axis=1, join='outer') # concatenate the create columns\n",
        "\t# just look at first subject\n",
        "\tdf=df.loc[df['id'] == individual]\n",
        "\tdf = df.drop('eeg_power', 1) # drop comma separated cell\n",
        "\t# df = df.drop('id', 1) # drop comma separated cell\n",
        "\treturn df\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "relax = df[df.label == 'relax']\n",
        "# df['label'] = df[\"label\"].astype('category')\n",
        "df['label'].value_counts()\n",
        "df['eeg_power'] = df.eeg_power.map(json.loads)\n",
        "\n",
        "individual_data=prepare_individual_data(df,1)\n",
        "\n",
        "def clean_labels(dd):\n",
        "\t# clean labels\n",
        "\tdd.loc[dd.label == 'math1', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math2', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math3', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math4', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math5', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math6', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math7', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math8', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math9', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math10', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math11', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math12', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'colorRound1-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'readyRound1', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound2', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound3', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound4', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound5', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'blink1', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink2', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink3', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink4', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink5', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'thinkOfItemsInstruction-ver1', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction1', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'musicInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'videoInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'mathInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'relaxInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'blinkInstruction', 'label'] = \"instruction\"\n",
        "\treturn dd\n",
        "\n",
        "cleaned_individual_data = clean_labels(individual_data)\n",
        "\n",
        "def drop_useless_labels(df):\n",
        "\t# drop unlabeled and everyone paired and others. leave only relax and math. \n",
        "\tdf = df[df.label != 'unlabeled']\n",
        "\tdf = df[df.label != 'everyone paired']\n",
        "\tdf = df[df.label != 'instruction']\n",
        "\tdf = df[df.label != 'blink']\n",
        "\tdf = df[df.label != 'ready']\n",
        "\tdf = df[df.label != 'colors']\n",
        "\tdf = df[df.label != 'thinkOfItems-ver1']\n",
        "\tdf = df[df.label != 'music']\n",
        "\tdf = df[df.label != 'video-ver1']\n",
        "\treturn df\n",
        "\n",
        "final_individual_full_data= drop_useless_labels(cleaned_individual_data)\n",
        "\n",
        "print(final_individual_full_data['label'].value_counts())\n",
        "\n",
        "print(final_individual_full_data.head())\n",
        "\n",
        "for i in range(9):\n",
        "\tcopy = final_individual_full_data\n",
        "\tcopy[0]=copy[0]+random.gauss(1,.1) # add noice to mean freq var\n",
        "\tfinal_individual_full_data=final_individual_full_data.append(copy,ignore_index=True) # make voice df 2x as big\n",
        "\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,final_individual_full_data.shape))\n",
        "\n",
        "\n",
        "def get_traintest_data(individualdata):\n",
        "\tlabel=individualdata.pop(\"label\") # pop off labels to new group\n",
        "\ttrain_labels = pd.get_dummies(label)\n",
        "\ttrain_labels = train_labels.values\n",
        "\tdf=individualdata.values\n",
        "\tx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n",
        "\t#so now we have predictors and y values, separated into test and train\n",
        "\tx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')\n",
        "\treturn x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_traintest_data(final_individual_full_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_mini_batch(x,y):\n",
        "\trows=np.random.choice(x.shape[0], 100)\n",
        "\treturn x[rows], y[rows]\n",
        "\n",
        "\n",
        "def trainNN(x_train, y_train,x_test,y_test,number_trials):\n",
        "\t# there are 8 features\n",
        "\t# place holder for inputs. feed in later\n",
        "\tx = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n",
        "\t# # # take 20 features  to 10 nodes in hidden layer\n",
        "\tw1 = tf.Variable(tf.random_normal([x_train.shape[1], 1000],stddev=.5,name='w1'))\n",
        "\t# # # add biases for each node\n",
        "\tb1 = tf.Variable(tf.zeros([1000]))\n",
        "\t# # calculate activations \n",
        "\thidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n",
        "\tw2 = tf.Variable(tf.random_normal([1000, y_train.shape[1]],stddev=.5,name='w2'))\n",
        "\tb2 = tf.Variable(tf.zeros([y_train.shape[1]]))\n",
        "\t# # placeholder for correct values \n",
        "\ty_ = tf.placeholder(\"float\", [None,y_train.shape[1]])\n",
        "\t# # #implement model. these are predicted ys\n",
        "\ty = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n",
        "\t# loss and optimization \n",
        "\tloss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(y, y_, name='xentropy')))\n",
        "\topt = tf.train.AdamOptimizer(learning_rate=.0005)\n",
        "\ttrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n",
        "\t# start session\n",
        "\tsess = tf.Session()\n",
        "\t# init all vars\n",
        "\tinit = tf.initialize_all_variables()\n",
        "\tsess.run(init)\n",
        "\tntrials = number_trials\n",
        "\tfor i in range(ntrials):\n",
        "\t    # get mini batch\n",
        "\t    a,b=get_mini_batch(x_train,y_train)\n",
        "\t    # run train step, feeding arrays of 100 rows each time\n",
        "\t    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n",
        "\t    if i%500 ==0:\n",
        "\t    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n",
        "\tcorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\tprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))\n",
        "\tans = sess.run(y, feed_dict={x: x_test})\n",
        "\tprint(y_test[0:3])\n",
        "\tprint(\"Correct prediction\\n\",ans[0:3])\n",
        "\n",
        "trainNN(x_train,y_train,x_test,y_test,50000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "17f994b3-d6a0-118a-f478-b2b26a30ca5f",
        "id": "CmBvM7iLqH4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7b6f7326-8595-a187-4bba-b22a61189d3a",
        "id": "Rte_49jOqH4Q",
        "colab_type": "text"
      },
      "source": [
        "This works about as well as Nick Merril's classification (a bit better)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9e2b448b-9abb-df63-dc47-09d3122f5337",
        "id": "mqqXFEHkqH4R",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Consolidate labels and run for individual subject. Try to classify more than 2 activities. ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fb63b4bf-cada-ebfc-f370-fb0e0b232f42",
        "id": "_f0liKI8qH4V",
        "colab_type": "text"
      },
      "source": [
        "Let's try to classify an individual subject's mode of thinking, with more possible categories. This is harder.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2eb2a596-e3a1-5681-e2dd-899a971eb721",
        "id": "GYapBcs1qH4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://github.com/aldomanpi/nnpainsensor/blob/4ce50563351e0f2b9817afa058f1478ea0a594f5/eeg-data.csv?raw=true'\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import json\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "\n",
        "def prepare_individual_data(df,individual):\n",
        "\t# drop unused features. just leave eeg_power and the label\n",
        "\tdf = df.drop('Unnamed: 0', 1)\n",
        "\t# df = df.drop('id', 1)\n",
        "\tdf = df.drop('indra_time', 1)\n",
        "\tdf = df.drop('browser_latency', 1)\n",
        "\tdf = df.drop('reading_time', 1)\n",
        "\tdf = df.drop('attention_esense', 1)\n",
        "\tdf = df.drop('meditation_esense', 1)\n",
        "\tdf = df.drop('raw_values', 1)\n",
        "\tdf = df.drop('signal_quality', 1)\n",
        "\tdf = df.drop('createdAt', 1)\n",
        "\tdf = df.drop('updatedAt', 1)\n",
        "\t# separate eeg power to multiple columns\n",
        "\tto_series = pd.Series(df['eeg_power']) # df to series\n",
        "\teeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n",
        "\tdf = pd.concat([df,eeg_features], axis=1) # concatenate the create columns\n",
        "\t# df = pd.concat([df,eeg_features], axis=1, join='outer') # concatenate the create columns\n",
        "\t# just look at first subject\n",
        "\tdf=df.loc[df['id'] == individual]\n",
        "\tdf = df.drop('eeg_power', 1) # drop comma separated cell\n",
        "\t# df = df.drop('id', 1) # drop comma separated cell\n",
        "\treturn df\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "relax = df[df.label == 'relax']\n",
        "# df['label'] = df[\"label\"].astype('category')\n",
        "df['label'].value_counts()\n",
        "df['eeg_power'] = df.eeg_power.map(json.loads)\n",
        "\n",
        "individual_data=prepare_individual_data(df,1)\n",
        "\n",
        "def clean_labels(dd):\n",
        "\t# clean labels\n",
        "\tdd.loc[dd.label == 'math1', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math2', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math3', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math4', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math5', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math6', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math7', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math8', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math9', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math10', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math11', 'label'] = \"math\"\n",
        "\tdd.loc[dd.label == 'math12', 'label'] = \"math\"\n",
        "\n",
        "\tdd.loc[dd.label == 'colorRound1-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound1-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound2-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound3-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound4-6', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-1', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-2', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-3', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-4', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-5', 'label'] = \"colors\"\n",
        "\tdd.loc[dd.label == 'colorRound5-6', 'label'] = \"colors\"\n",
        "\n",
        "\tdd.loc[dd.label == 'readyRound1', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound2', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound3', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound4', 'label'] = \"ready\"\n",
        "\tdd.loc[dd.label == 'readyRound5', 'label'] = \"ready\"\n",
        "\n",
        "\tdd.loc[dd.label == 'blink1', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink2', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink3', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink4', 'label'] = \"blink\"\n",
        "\tdd.loc[dd.label == 'blink5', 'label'] = \"blink\"\n",
        "\n",
        "\tdd.loc[dd.label == 'thinkOfItemsInstruction-ver1', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction1', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'musicInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'videoInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'mathInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'relaxInstruction', 'label'] = \"instruction\"\n",
        "\tdd.loc[dd.label == 'blinkInstruction', 'label'] = \"instruction\"\n",
        "\n",
        "\treturn dd\n",
        "\n",
        "cleaned_individual_data = clean_labels(individual_data)\n",
        "\n",
        "def drop_useless_labels(df):\n",
        "\t# drop unlabeled and everyone paired.\n",
        "\tdf = df[df.label != 'unlabeled']\n",
        "\tdf = df[df.label != 'everyone paired']\n",
        "\treturn df\n",
        "\n",
        "final_individual_full_data= drop_useless_labels(cleaned_individual_data)\n",
        "\n",
        "print(final_individual_full_data['label'].value_counts())\n",
        "\n",
        "for i in range(9):\n",
        "\tcopy = final_individual_full_data\n",
        "\tcopy[0]=copy[0]+random.gauss(1,.1) # add noice to mean freq var\n",
        "\tfinal_individual_full_data=final_individual_full_data.append(copy,ignore_index=True) # make voice df 2x as big\n",
        "\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,final_individual_full_data.shape))\n",
        "\n",
        "\n",
        "def get_traintest_data(individualdata):\n",
        "\tlabel=individualdata.pop(\"label\") # pop off labels to new group\n",
        "\ttrain_labels = pd.get_dummies(label)\n",
        "\ttrain_labels = train_labels.values\n",
        "\tdf=individualdata.values\n",
        "\tx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n",
        "\t#so now we have predictors and y values, separated into test and train\n",
        "\tx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')\n",
        "\treturn x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = get_traintest_data(final_individual_full_data)\n",
        "\n",
        "\n",
        "def get_mini_batch(x,y):\n",
        "\trows=np.random.choice(x.shape[0], 50)\n",
        "\treturn x[rows], y[rows]\n",
        "\n",
        "\n",
        "def trainNN(x_train, y_train,x_test,y_test,number_trials):\n",
        "\t# there are 8 features\n",
        "\t# place holder for inputs. feed in later\n",
        "\tx = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n",
        "\t# # # take 20 features  to 10 nodes in hidden layer\n",
        "\tw1 = tf.Variable(tf.random_normal([x_train.shape[1], 1000],stddev=.5,name='w1'))\n",
        "\t# # # add biases for each node\n",
        "\tb1 = tf.Variable(tf.zeros([1000]))\n",
        "\t# # calculate activations \n",
        "\thidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n",
        "\tw2 = tf.Variable(tf.random_normal([1000, y_train.shape[1]],stddev=.5,name='w2'))\n",
        "\tb2 = tf.Variable(tf.zeros([y_train.shape[1]]))\n",
        "\t# # placeholder for correct values \n",
        "\ty_ = tf.placeholder(\"float\", [None,y_train.shape[1]])\n",
        "\t# # #implement model. these are predicted ys\n",
        "\ty = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n",
        "\t# loss and optimization \n",
        "\tloss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(y, y_, name='xentropy')))\n",
        "\topt = tf.train.AdamOptimizer(learning_rate=.002)\n",
        "\ttrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n",
        "\t# start session\n",
        "\tsess = tf.Session()\n",
        "\t# init all vars\n",
        "\tinit = tf.initialize_all_variables()\n",
        "\tsess.run(init)\n",
        "\tntrials = number_trials\n",
        "\tfor i in range(ntrials):\n",
        "\t    # get mini batch\n",
        "\t    a,b=get_mini_batch(x_train,y_train)\n",
        "\t    # run train step, feeding arrays of 100 rows each time\n",
        "\t    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n",
        "\t    if i%500 ==0:\n",
        "\t    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n",
        "\tcorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\tprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))\n",
        "\n",
        "\n",
        "\n",
        "trainNN(x_train,y_train,x_test,y_test,50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a4792f2e-ad24-f60c-1d9a-207d715a90a1",
        "id": "GfHnWBi2qH4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}